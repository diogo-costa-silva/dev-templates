# Data Analytics Project Methodology Guide & Template

This repository follows a structured methodology for data analytics projects, based on the comprehensive guide/checklist available [here](checklist.md).  
The objective is to ensure **reproducibility**, **clarity**, and **best practices** regardless of data type (numeric, categorical, temporal, text, geospatial, etc.).

## 📂 Project Structure

```text
data-analytics-project/
├─ .venv/
├─ data/              # raw/, interim/, processed/
│  ├─ raw/            # Original datasets
│  ├─ interim/        # Partially processed data
│  └─ processed/      # Analysis-ready data
├─ notebooks/         # 01_xxx.ipynb, 02_xxx.ipynb...
├─ scripts/           # Standalone processing scripts
├─ sql/               # 01_exploration.sql, 02_cleaning.sql...
├─ src/               # Reusable Python modules (ETL, utils, pipelines)
├─ reports/           # figures/, dashboards/, final reports
│  ├─ figures/
│  └─ dashboards/
├─ config/            # .env.example, params.yml
├─ tests/             # Automated tests
├─ .env
├─ .gitignore
├─ main.py
├─ pyproject.toml     # Generated by UV
├─ README.md          # Project description
└─ LICENSE
```

## 🛠 Environment Setup

1. **Clone repository**
   ```bash
   git clone <repo-url>
   cd <repo-name>
   ```

2. **Create virtual environment and install dependencies** (preferably with UV)
   ```bash
   # Using UV (recommended)
   uv venv
   source .venv/bin/activate  # Linux/Mac
   .venv\Scripts\activate     # Windows
   uv pip install -r requirements.txt

   # Alternative with standard Python
   python -m venv .venv
   source .venv/bin/activate  # Linux/Mac
   .venv\Scripts\activate     # Windows
   pip install -r requirements.txt
   ```

3. **Configure environment variables**
   ```bash
   cp config/.env.example .env
   # Edit .env as needed
   ```

4. **Execute pipeline** (example with Makefile)
   ```bash
   make run
   ```

## 🚀 Project Methodology

This project follows an 8-phase **CRISP-DM adapted methodology** for Data Analytics:

### 1. Project Setup
**Objective:** Ensure reproducibility, organization, and security from the start.  
**Deliverables:** Versioned repository, folder structure, configured environment.

### 2. Initial Data Overview
**Objective:** Rapid inventory of available datasets to understand what we have before formulating business questions.  
**Deliverables:** Dataset inventory, initial quality assessment, high-level relationship mapping.

### 3. Business Understanding
**Objective:** Align problem, expected value, and success criteria based on available data.  
**Deliverables:** Problem statement, hypotheses, KPIs, acceptance criteria.

### 4. Data Understanding
**Objective:** Detailed technical analysis of data quality, structure, and content using Python/SQL tools.  
**Deliverables:** Data understanding report, data dictionary, identified problems and opportunities.

### 5. Database Design & Integration Planning
**Objective:** Design coherent data structure and integration strategy before final preparation.  
**Deliverables:** ER diagram, integration strategy, join plan, data dictionary.

### 6. Data Preparation
**Objective:** Transform data into clean, analyzable structures.  
**Deliverables:** Clean datasets, reproducible scripts, transformation log.

### 7. Exploratory Data Analysis (EDA)
**Objective:** Generate insights and evidence for decisions.  
**Deliverables:** Reproducible notebook, explanatory graphics, top insights.

### 8. Insights & Reporting
**Objective:** Transform analyses into decisions and actions.  
**Deliverables:** Concise report, interpretive graphics, recommendations.

For detailed implementation of each phase, see the [complete checklist](checklist.md).

## 📊 Essential Data Analysis Toolkit

### Quick Data Overview
```python
# Essential commands for initial assessment
df.shape                    # Dimensions (rows, columns)
df.info()                   # Data types and memory usage
df.head(10), df.tail(10)    # First/last rows
df.describe()               # Statistical summary
df.isnull().sum()           # Missing values count
df.nunique()                # Unique values per column
```

### Data Quality Assessment
```python
# Missing values analysis
df.isnull().mean() * 100    # Missing percentage by column

# Duplicates
df.duplicated().sum()       # Total duplicate rows
df.duplicated(subset=['key']).sum()  # Duplicates by key

# Data type validation
df.dtypes                   # Current data types
pd.to_datetime(df['date_col'])  # Convert to datetime
pd.to_numeric(df['num_col'], errors='coerce')  # Convert to numeric
```

### Basic EDA Patterns
```python
# Numerical variables
df.hist(bins=20, figsize=(15, 10))     # Distributions
df.boxplot(figsize=(15, 5))            # Outlier detection
df.corr()                              # Correlation matrix

# Categorical variables
df['col'].value_counts()               # Frequency counts
df['col'].value_counts(normalize=True) # Percentages

# Relationships
pd.crosstab(df['cat1'], df['cat2'])    # Categorical × Categorical
df.groupby('category')['numeric'].describe()  # Categorical × Numeric
```

## 🔍 Advanced Analysis Techniques

The project includes advanced techniques for:
- **Database Modeling & ERD**: Automatic relationship analysis and foreign key validation
- **Performance Optimization**: Memory usage reduction and execution time profiling
- **Data Quality Assurance**: Automated quality tests and business rules validation
- **Advanced EDA**: Correlation analysis, categorical associations, automatic segmentation
- **SQL for Data Modeling**: CTEs, window functions, complex join validation

See the [complete toolkit](checklist.md#essential-toolkit---advanced-commands) for implementation details.

## 📋 Quick Reference by Data Type

| Type | Validation | Preparation | EDA |
|------|------------|-------------|-----|
| Numeric | intervals, outliers, units | imputation, scaling | boxplot, correlation |
| Categorical | rare levels, encoding | normalize, group | top-k, crosstab |
| Datetime | timezone, gaps | temporal features | trend, seasonality |
| Text | encoding, PII | basic cleaning | frequencies, lengths |

## 🎯 Definition of Done by Phase

- **Setup:** Environment configured and structure created
- **Initial Overview:** Complete inventory and problems identified  
- **Business Understanding:** KPIs and criteria approved
- **Data Understanding:** Detailed technical analysis completed
- **Database Design:** ER diagram and integration strategy defined
- **Data Preparation:** Clean datasets with validations
- **EDA:** Prioritized insights and quantified evidence
- **Reporting:** 1-pager + artifacts for decision-making

## 📝 Contributing Guidelines

- Maintain consistent file naming (`01_filename.ipynb`)
- Store raw data only in `data/raw/`
- Never commit sensitive data or PII
- Follow the 8-phase methodology sequence
- Document all transformations with reasoning
- Include validation tests for data quality

## 📄 Documentation

- **[Complete Methodology Checklist](checklist.md)** - Detailed implementation guide for each phase
- **[Advanced Toolkit](checklist.md#essential-toolkit---advanced-commands)** - Python/SQL code examples for complex analysis

## 📜 License

This project is distributed under the MIT License. See [LICENSE](LICENSE) for more details.