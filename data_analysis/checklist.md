# Data Analytics Project Methodology Guide/Checklist (Python/SQL)
*Updated on 2025-08-12*  
This guide is data-agnostic and focuses on reproducible steps, best practices, and clear decisions for those starting in Data Analytics.

**Sequential workflow (CRISP-DM adapted for Data Analytics):**

0. Project Setup
1. Initial Data Overview
2. Business Understanding 
3. Data Understanding
4. Pre-EDA (Sanity Check)
5. Data Modeling + DB Design & Integration Planning
6. Data Preparation
7. Exploratory Data Analysis (EDA)
8. Insights & Reporting

---

## 0) Project Setup
**Objective:** ensure reproducibility, organization, and security from the start.  
**Deliverables:** versioned repository, folder structure, configured environment.

### Checklist

- [ ] **Create Git repository** and minimal structure:
  ```text
  data-analytics-project/
  ├─ .venv/
  ├─ data/              # raw/, interim/, processed/
  │  ├─ raw/
  │  ├─ interim/
  │  └─ processed/
  ├─ notebooks/         # 01_xxx.ipynb, 02_xxx.ipynb...
  ├─ scripts/
  ├─ sql/               # 01_exploration.sql, 02_cleaning.sql...
  ├─ src/               # utils.py, pipelines/
  ├─ reports/           # figures/, dashboards/
  ├─ config/            # .env.example, params.yml
  ├─ .env
  ├─ .gitignore
  ├─ main.py
  ├─ pyproject.toml     # generated by UV
  ├─ README.md
  └─ LICENSE
  ```

- [ ] **Define Python environment** (preferably UV) and dependencies.
- [ ] **Create `.env.example`** (without secrets) and validate variables.
- [ ] **Define conventions:** file names, notebook prefixes, commits.

---

## 1) Initial Data Overview
**Objective:** rapid inventory of available datasets to understand what we have before formulating business questions.  
**Deliverables:** dataset inventory, initial quality assessment, high-level relationship mapping.

### 1.1) Dataset Inventory

- [ ] **List all files/tables** with:
  - Name, size (rows × columns), format
  - Last update date and source/origin
  - Data owner/responsible person

```python
# Essential commands for inventory
df.shape                    # Dimensions (rows, columns)
df.info()                   # Data types and memory usage
df.head(10), df.tail(10)    # First/last rows
df.columns.tolist()         # Complete column list
```

### 1.2) High-Level Quality Check

- [ ] **Missing values:** `df.isnull().sum()` and `df.isnull().mean() * 100`
- [ ] **Duplicates:** `df.duplicated().sum()` and by business key
- [ ] **Cardinality:** `df.nunique()` and uniqueness ratio
- [ ] **Data types:** validate if correct (`df.dtypes`)
- [ ] **Extreme values:** `df.describe()` to identify obvious issues

### 1.3) Initial Relationship Identification

- [ ] **Common variables** across datasets (same/similar names)
- [ ] **Potential join keys** (foreign keys)
- [ ] **Relationship cardinality** (1:1, 1:N, N:N)
- [ ] **Identify main dataset** (fact table) vs auxiliary (dimensions)

---

## 2) Business Understanding
**Objective:** align problem, expected value, and success criteria based on available data.  
**Deliverables:** problem statement, hypotheses, KPIs, acceptance criteria.

### Key Questions
- What business decision needs to be made and by whom?
- Which KPIs/OKRs are affected? What's the baseline and target?
- What priority hypotheses should we test?
- What specific questions do we want to ask the data? For what purpose?

### Checklist

- [ ] **Define Problem Statement** (1–3 clear sentences).
- [ ] **Map main KPIs** and derived metrics.
- [ ] **List hypotheses** and expected impact (↑/↓ KPI).
- [ ] **Success/failure criteria** and target decision.
- [ ] **Formulate specific business questions** for the data.
- [ ] **Define constraints** (time, compliance, minimum quality).

---

## 3) Data Understanding
**Objective:** detailed technical analysis of data quality, structure, and content using Python/SQL tools.  
**Deliverables:** data understanding report, data dictionary, identified problems and opportunities.

### 3.1) Metadata and Data Origin

- [ ] **Document data sources:**
  - Data owner/responsible person
  - Publication/creation date
  - Update frequency (SLA)
  - Data generation process
  - Dependencies and upstream systems

### 3.2) Structure and Dimensions

- [ ] **Basic dimensional analysis:**
  - `df.shape` - How many rows × columns?
  - `df.info()` - Data types and memory usage
  - `df.columns.tolist()` - Complete column list
- [ ] **Naming validation:**
  - Do column titles make sense?
  - Consistent conventions?
  - Need to rename columns?

### 3.3) Data Types and Formats

- [ ] **Check data types** (`df.dtypes`):
  - Numeric: int vs float, adequate precision?
  - Strings: object vs category for optimization?
  - Dates: datetime vs object string?
  - Booleans: bool vs int/string?
- [ ] **Validate specific formats:**
  - Dates: consistent format, timezone
  - Numbers: decimal separators, units
  - Text: encoding, case sensitivity
  - IDs: pattern, fixed/variable length

### 3.4) Content Analysis

- [ ] **Uniqueness and cardinality** per column:
  - `df.nunique()` - How many unique values?
  - `df.nunique() / len(df)` - Uniqueness ratio
  - Identify primary key candidates
  - Identify categorical columns (low cardinality)
- [ ] **Statistical classification** of each column:
  - Numeric continuous vs discrete
  - Categorical nominal vs ordinal
  - Temporal: point vs interval
  - Identifiers/keys

### 3.5) Data Quality Assessment

- [ ] **Missing values analysis** (`df.isnull().sum()`):
  - Missing pattern: random vs systematic?
  - % missing per column: `df.isnull().mean()`
  - Do missing values represent "zero" or "unknown"?
  - Decide strategy: remove, impute, or keep as category
- [ ] **Preliminary outlier detection:**
  - `df.describe()` - suspicious min/max?
  - Impossible values (negative ages, future dates)
  - IQR method: values outside Q1-1.5×IQR or Q3+1.5×IQR

### 3.6) Duplicates and Consistency

- [ ] **Duplicate analysis:**
  - `df.duplicated().sum()` - 100% duplicate rows
  - Duplicates by business key (`df.duplicated(subset=['key']).sum()`)
  - Investigate if duplicates are legitimate or errors
- [ ] **Internal consistency:**
  - Do calculated fields match components? (e.g., total = sum(parts))
  - Are logical relationships valid? (e.g., end_date > start_date)
  - Are expected ranges respected?

---

## 4) Database Design & Integration Planning
**Objective:** design coherent data structure and integration strategy before final preparation.  
**Deliverables:** ER diagram, integration strategy, join plan, data dictionary.

### 4.1) Entity and Attribute Mapping

- [ ] **Map entities and attributes:**
  - Identify fact tables (events, metrics) vs dimension tables (references, categories)
  - Document each attribute (name, type, description, source)
- [ ] **Define primary and foreign keys:**
  - Validate uniqueness: `df.groupby(key).size().max() == 1`
  - Test coverage between tables
- [ ] **Model relationships:**
  - 1:1, 1:N, N:N (use bridge tables if necessary)
  - Expected cardinality
  - Document referential integrity rules

### 4.2) Integration Strategy

- [ ] **Join planning:**
  - Join order (fact table as base)
  - Join types (inner/left/outer) based on business rules
  - Strategy for orphaned records
- [ ] **Handle naming conflicts:**
  - Columns with same name: use explicit suffixes
  - Conflicting values: define source of truth (SSOT)
- [ ] **Create simple ER diagram** (dbdiagram.io, draw.io, or Mermaid)

```sql
-- Foreign key validation before joins
SELECT 
    COUNT(*) as total_records,
    COUNT(t2.id) as matched_records,
    COUNT(*) - COUNT(t2.id) as orphan_records,
    ROUND(100.0 * COUNT(t2.id) / COUNT(*), 2) as match_rate
FROM table1 t1
LEFT JOIN table2 t2 ON t1.foreign_key = t2.id;
```

### 4.3) Normalization and Denormalization Decisions

- [ ] **Eliminate unnecessary redundancy**
- [ ] **Create calculated columns only when necessary**
- [ ] **Plan aggregation levels** for analysis
- [ ] **Document business rules** and constraints

---

## 5) Data Preparation
**Objective:** transform data into clean, analyzable structures.  
**Deliverables:** clean datasets, reproducible scripts, transformation log.

### 5.1) Essential Cleaning

- [ ] **Format standardization:**
  ```python
  # Basic cleaning
  df['col'].str.strip()              # Remove whitespace
  df['col'].str.lower()              # Lowercase
  pd.to_datetime(df['date_col'])     # String to datetime
  pd.to_numeric(df['num_col'], errors='coerce')  # String to numeric
  ```

- [ ] **Missing values treatment:**
  ```python
  # Strategies by type
  df['num_col'].fillna(df['num_col'].median())  # Numeric: median
  df['cat_col'].fillna('Unknown')               # Categorical: category
  df.fillna(method='ffill')                     # Temporal: forward fill
  ```

### 5.2) Join Operations

- [ ] **Prepare keys for joins:**
  - Ensure same data types
  - Validate coverage between tables
- [ ] **Execute joins with validation:**
  ```python
  # Validate before/after join
  print(f"Before: {len(df1)} records")
  df_merged = df1.merge(df2, on='key', how='left')
  print(f"After: {len(df_merged)} records")
  print(f"Match rate: {df_merged['key_from_df2'].notna().mean():.2%}")
  ```

### 5.3) Basic Feature Engineering

- [ ] **Derive calculated variables:**
  ```python
  # Temporal features
  df['year'] = df['date_col'].dt.year
  df['is_weekend'] = df['date_col'].dt.weekday >= 5
  df['days_since'] = (pd.Timestamp.now() - df['date_col']).dt.days
  
  # Ratios and flags
  df['ratio'] = df['num1'] / df['num2']
  df['is_high_value'] = df['value'] > df['value'].quantile(0.8)
  ```

### 5.4) Validation and Testing

- [ ] **Post-cleaning consistency tests:**
  - Are business rules still valid?
  - Are expected distributions maintained?
- [ ] **Document transformations** with reasoning
- [ ] **Save "gold" version** of clean data

---

## 6) Exploratory Data Analysis (EDA)
**Objective:** generate insights and evidence for decisions.  
**Deliverables:** reproducible notebook, explanatory graphics, top insights.

### 6.1) Essential Univariate Analysis

```python
# Numerical variables
df.describe()                           # Descriptive statistics
df.hist(bins=20, figsize=(15, 10))     # Histograms
df.boxplot(figsize=(15, 5))            # Boxplots for outliers

# Categorical variables
df['col'].value_counts()                # Frequency table
df['col'].value_counts(normalize=True)  # Percentages
df['col'].value_counts().plot.bar()     # Visualization
```

### 6.2) Key Relationship Analysis

```python
# Correlations
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True)

# Numeric × Categorical
sns.boxplot(x='category', y='numeric', data=df)
df.groupby('category')['numeric'].describe()

# Categorical × Categorical
pd.crosstab(df['cat1'], df['cat2'])
pd.crosstab(df['cat1'], df['cat2'], normalize='index')
```

### 6.3) Temporal Analysis (if applicable)

```python
# Time series
df.set_index('date').resample('M').mean()           # Monthly aggregation
df.set_index('date')['value'].rolling(30).mean()    # 30-day moving average

# Seasonal patterns
df.groupby(df['date'].dt.dayofweek)['value'].mean() # By day of week
df.groupby(df['date'].dt.month)['value'].mean()     # By month
```

### 6.4) Segmentation and Insights

- [ ] **Analysis by quartiles/percentiles**
- [ ] **Segments by relevant business logic**
- [ ] **Top 5-7 most important insights** quantified
- [ ] **Link insights** to original business questions

---

## 7) Insights & Reporting
**Objective:** transform analyses into decisions and actions.  
**Deliverables:** concise report, interpretive graphics, recommendations.

### Report Structure (1-pager)
1. **Context & Objective**
2. **Data & Quality** (summary)
3. **3–5 Main Insights** (each with "So what?")
4. **Recommendations and next steps**
5. **Limitations** and appendices

### Checklist

- [ ] **Clear storyline:** context → question → evidence → implication → recommendation
- [ ] **Graphics with interpretive titles** and call-outs
- [ ] **Quantify expected impact** and uncertainty
- [ ] **Document limitations** (bias, quality, causality)
- [ ] **Reproducible package:** notebooks + SQL + README

<br>

---
---
---

<br>

## Essential Toolkit - Advanced Commands

### Database Modeling & ERD
```python
# Automatic relationship analysis
def analyze_potential_joins(df1, df2):
    """Identifies possible join columns between DataFrames"""
    common_cols = set(df1.columns) & set(df2.columns)
    for col in common_cols:
        overlap = len(set(df1[col]) & set(df2[col]))
        print(f"{col}: {overlap} common values")

# Foreign key validation
def validate_foreign_key(parent_df, child_df, parent_key, foreign_key):
    """Validates referential integrity"""
    orphans = child_df[~child_df[foreign_key].isin(parent_df[parent_key])]
    print(f"Orphaned records: {len(orphans)} ({len(orphans)/len(child_df):.2%})")
    return orphans

# Automatic cardinality detection
def detect_cardinality(df1, df2, key1, key2):
    """Detects relationship type between tables"""
    df1_unique = df1[key1].nunique()
    df2_unique = df2[key2].nunique()
    df1_total = len(df1)
    df2_total = len(df2)
    
    if df1_unique == df1_total and df2_unique == df2_total:
        return "1:1"
    elif df1_unique == df1_total:
        return "1:N"
    elif df2_unique == df2_total:
        return "N:1"
    else:
        return "N:N"
```

### Performance & Memory Optimization
```python
# Memory usage reduction
def optimize_dtypes(df):
    """Optimizes data types to reduce memory usage"""
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type != 'object':
            c_min = df[col].min()
            c_max = df[col].max()
            
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
        else:
            # Convert to category if few unique values
            if df[col].nunique() / len(df) < 0.5:
                df[col] = df[col].astype('category')
    
    return df

# Performance profiling
import time
def time_operation(func, *args, **kwargs):
    """Measures execution time of operations"""
    start = time.time()
    result = func(*args, **kwargs)
    end = time.time()
    print(f"Operation executed in {end - start:.2f} seconds")
    return result

# Memory usage check
def memory_usage(df):
    """Shows detailed memory usage by column"""
    mem_usage = df.memory_usage(deep=True)
    return mem_usage.sort_values(ascending=False)
```

### Data Validation & Quality Assurance
```python
# Automated quality tests
def data_quality_report(df):
    """Generates comprehensive data quality report"""
    report = {
        'total_rows': len(df),
        'total_columns': len(df.columns),
        'missing_values': df.isnull().sum().to_dict(),
        'missing_percentage': (df.isnull().sum() / len(df) * 100).to_dict(),
        'duplicated_rows': df.duplicated().sum(),
        'unique_values': df.nunique().to_dict(),
        'data_types': df.dtypes.to_dict()
    }
    return report

# Business rules validation
def validate_business_rules(df, rules):
    """
    Validates custom business rules
    rules = {'age': lambda x: x >= 0, 'email': lambda x: '@' in str(x)}
    """
    violations = {}
    for column, rule in rules.items():
        if column in df.columns:
            violations[column] = ~df[column].apply(rule)
    return violations

# Anomaly detection
from sklearn.ensemble import IsolationForest
def detect_anomalies(df, columns, contamination=0.1):
    """Detects anomalies using Isolation Forest"""
    iso_forest = IsolationForest(contamination=contamination)
    anomalies = iso_forest.fit_predict(df[columns])
    return df[anomalies == -1]
```

### Advanced EDA Techniques
```python
# Advanced correlation analysis
def advanced_correlation_analysis(df):
    """Correlation analysis with different methods"""
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    correlations = {
        'pearson': df[numeric_cols].corr(method='pearson'),
        'spearman': df[numeric_cols].corr(method='spearman'),
        'kendall': df[numeric_cols].corr(method='kendall')
    }
    return correlations

# Categorical association analysis
from scipy.stats import chi2_contingency
def categorical_association(df, col1, col2):
    """Calculates association between categorical variables"""
    crosstab = pd.crosstab(df[col1], df[col2])
    chi2, p_value, dof, expected = chi2_contingency(crosstab)
    
    # Cramer's V
    n = crosstab.sum().sum()
    cramers_v = np.sqrt(chi2 / (n * (min(crosstab.shape) - 1)))
    
    return {
        'chi2': chi2,
        'p_value': p_value,
        'cramers_v': cramers_v,
        'significant': p_value < 0.05
    }

# Automatic segmentation
from sklearn.cluster import KMeans
def auto_segmentation(df, features, n_clusters=5):
    """Automatic segmentation using K-means"""
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    df['segment'] = kmeans.fit_predict(df[features])
    
    # Profile each segment
    segment_profiles = df.groupby('segment')[features].agg(['mean', 'std', 'count'])
    return df, segment_profiles
```

### SQL for Data Modeling
```sql
-- Common Table Expressions (CTEs) for complex analysis
WITH customer_metrics AS (
    SELECT 
        customer_id,
        COUNT(*) as total_orders,
        SUM(order_value) as total_spent,
        AVG(order_value) as avg_order_value,
        MAX(order_date) as last_order_date
    FROM orders
    GROUP BY customer_id
),
customer_segments AS (
    SELECT *,
        CASE 
            WHEN total_spent > 1000 THEN 'High Value'
            WHEN total_spent > 500 THEN 'Medium Value'
            ELSE 'Low Value'
        END as segment
    FROM customer_metrics
)
SELECT segment, COUNT(*) as customers, AVG(total_spent) as avg_spent
FROM customer_segments
GROUP BY segment;

-- Essential window functions
SELECT 
    customer_id,
    order_date,
    order_value,
    -- Ranking
    ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_date) as order_sequence,
    RANK() OVER (ORDER BY order_value DESC) as value_rank,
    
    -- Moving aggregations
    SUM(order_value) OVER (PARTITION BY customer_id ORDER BY order_date 
                          ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as cumulative_spent,
    AVG(order_value) OVER (PARTITION BY customer_id ORDER BY order_date 
                          ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) as moving_avg_3orders,
    
    -- Lags and Leads
    LAG(order_value, 1) OVER (PARTITION BY customer_id ORDER BY order_date) as previous_order_value,
    LEAD(order_date, 1) OVER (PARTITION BY customer_id ORDER BY order_date) as next_order_date
FROM orders;

-- Complex join validation
SELECT 
    'customers' as table_name,
    COUNT(*) as total_records,
    COUNT(DISTINCT customer_id) as unique_customers,
    CASE WHEN COUNT(*) = COUNT(DISTINCT customer_id) THEN 'PK Valid' ELSE 'PK Invalid' END as pk_status
FROM customers

UNION ALL

SELECT 
    'orders' as table_name,
    COUNT(*) as total_records,
    COUNT(DISTINCT customer_id) as unique_customers,
    CONCAT(COUNT(DISTINCT customer_id), ' of ', (SELECT COUNT(*) FROM customers), ' customers have orders') as fk_coverage
FROM orders;
```

### Export & Reporting
```python
# Automatic report generation
def generate_data_report(df, output_path='data_report.html'):
    """Generates automatic HTML data report"""
    import pandas_profiling
    profile = pandas_profiling.ProfileReport(df, title='Data Analysis Report')
    profile.to_file(output_path)
    print(f"Report generated: {output_path}")

# Export to multiple formats
def export_analysis_results(df, insights_dict, base_name='analysis'):
    """Exports results to different formats"""
    # Excel with multiple sheets
    with pd.ExcelWriter(f'{base_name}.xlsx') as writer:
        df.to_excel(writer, sheet_name='Data', index=False)
        pd.DataFrame(insights_dict.items(), columns=['Insight', 'Value']).to_excel(
            writer, sheet_name='Insights', index=False)
    
    # CSV
    df.to_csv(f'{base_name}_data.csv', index=False)
    
    # JSON for insights
    import json
    with open(f'{base_name}_insights.json', 'w') as f:
        json.dump(insights_dict, f, indent=2)

# Simple dashboard with Plotly
import plotly.graph_objects as go
from plotly.subplots import make_subplots

def create_summary_dashboard(df, numeric_cols, categorical_cols):
    """Creates simple dashboard with main metrics"""
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=('Correlation', 'Distributions', 'Categorical', 'Temporal'),
        specs=[[{"type": "xy"}, {"type": "xy"}],
               [{"type": "xy"}, {"type": "xy"}]]
    )
    
    # Correlation heatmap
    corr_matrix = df[numeric_cols].corr()
    fig.add_trace(
        go.Heatmap(z=corr_matrix.values, x=corr_matrix.columns, y=corr_matrix.columns),
        row=1, col=1
    )
    
    # Add other charts...
    fig.update_layout(height=800, title_text="Data Analysis Dashboard")
    return fig
```

### Troubleshooting Common Issues
```python
# Debugging problematic joins
def debug_join_issues(df1, df2, key):
    """Diagnoses common join problems"""
    print("=== Join Debugging ===")
    
    # Check data types
    print(f"Key type in df1: {df1[key].dtype}")
    print(f"Key type in df2: {df2[key].dtype}")
    
    # Check null values
    print(f"Nulls in df1[{key}]: {df1[key].isnull().sum()}")
    print(f"Nulls in df2[{key}]: {df2[key].isnull().sum()}")
    
    # Check overlap
    overlap = set(df1[key]) & set(df2[key])
    print(f"Common values: {len(overlap)}")
    print(f"Overlap rate: {len(overlap)/len(set(df1[key])):.2%}")
    
    # Show examples of non-matching values
    df1_only = set(df1[key]) - set(df2[key])
    df2_only = set(df2[key]) - set(df1[key])
    print(f"Only in df1: {list(df1_only)[:5]}")
    print(f"Only in df2: {list(df2_only)[:5]}")

# Encoding issues resolution
def fix_encoding_issues(df, text_columns):
    """Resolves common encoding problems"""
    for col in text_columns:
        if col in df.columns:
            # Decoding attempt
            try:
                df[col] = df[col].str.encode('latin1').str.decode('utf8')
            except:
                print(f"Could not fix encoding for column {col}")
    return df

# Large dataset processing
def process_large_dataset(file_path, chunk_size=10000, processing_func=None):
    """Processes large datasets in chunks to avoid memory errors"""
    results = []
    
    for chunk in pd.read_csv(file_path, chunksize=chunk_size):
        if processing_func:
            processed_chunk = processing_func(chunk)
        else:
            processed_chunk = chunk
        
        results.append(processed_chunk)
        
        # Memory monitoring
        import psutil
        memory_percent = psutil.virtual_memory().percent
        if memory_percent > 80:
            print(f"Warning: Memory usage at {memory_percent:.1f}%")
    
    return pd.concat(results, ignore_index=True)

# Time series consistency validation
def validate_time_series(df, date_col, value_col):
    """Validates consistency in time series"""
    issues = []
    
    # Check temporal duplicates
    duplicates = df[df.duplicated(subset=[date_col], keep=False)]
    if len(duplicates) > 0:
        issues.append(f"Duplicate dates: {len(duplicates)} records")
    
    # Check temporal gaps
    df_sorted = df.sort_values(date_col)
    date_diffs = df_sorted[date_col].diff()
    median_diff = date_diffs.median()
    large_gaps = date_diffs[date_diffs > median_diff * 3]
    if len(large_gaps) > 0:
        issues.append(f"Large temporal gaps: {len(large_gaps)} occurrences")
    
    # Check temporal outliers
    if df[value_col].dtype in ['int64', 'float64']:
        Q1, Q3 = df[value_col].quantile([0.25, 0.75])
        IQR = Q3 - Q1
        outliers = df[(df[value_col] < Q1 - 1.5*IQR) | (df[value_col] > Q3 + 1.5*IQR)]
        if len(outliers) > 0:
            issues.append(f"Outliers detected: {len(outliers)} values")
    
    return issues
```

---

## Quick Reference by Data Type
| Type | Validation | Preparation | EDA | 
|---|---|---|---|
| Numeric | intervals, outliers, units | imputation, scaling | boxplot, correlation |
| Categorical | rare levels, encoding | normalize, group | top-k, crosstab |
| Datetime | timezone, gaps | temporal features | trend, seasonality |
| Text | encoding, PII | basic cleaning | frequencies, lengths |

---

## Definition of Done by Phase
- **Setup:** Environment configured and structure created
- **Initial Overview:** Complete inventory and problems identified
- **Business Understanding:** KPIs and criteria approved
- **Data Understanding:** Detailed technical analysis completed
- **Database Design:** ER diagram and integration strategy defined
- **Data Preparation:** Clean datasets with validations
- **EDA:** Prioritized insights and quantified evidence
- **Reporting:** 1-pager + artifacts for decision-making